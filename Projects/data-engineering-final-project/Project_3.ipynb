{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Report: Peter Morgan, Bruce Lam, Eda Kavlakoglu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    " The data engineering court at Renaissance Games is pleased to present our lords and ladies with an analytics pipeline to keep a watchful eye on the activities of the merchants within the market and also to note the comings of knights and ladies in the guilds. \n",
    " \n",
    " Our sorcery (stack) of choice is as follows:\n",
    " \n",
    " - Apache Bench - \"game client\" sending  player events into data pipeline\n",
    " - Flask - app that runs the game Application Programming Interface\n",
    " - Kafka - platform for ingesting streaming data and passing to downstream applications\n",
    " - Spark - tool to filter and transform data and push to or pull from HDFS (Hadoop Distributed File System)\n",
    " - Hadoop - distributed file system for managing parquet files\n",
    " - Hive - intermediary to track and agree upon schema and create tables\n",
    " - Presto - query tool for summarizing and reporting analytics on purchases and guild activity\n",
    " \n",
    "Prithee see the current summary of activity in the market and guilds.\n",
    " \n",
    " ### Sword sale summary:\n",
    " \n",
    " Most popular sword: sharp = 154, normal = 154\n",
    " \n",
    " Total swords purchased: 308\n",
    " \n",
    " ### Guild summary\n",
    " \n",
    " Kavlakoglu guild member count: 199\n",
    " \n",
    " Morgan guild member count: 201\n",
    " \n",
    " Lam guild member count: 216\n",
    " \n",
    " For a detailed breakdown of randomly and manually generated events, prithee see the Business Analytics Questions section\n",
    " \n",
    "Gramercy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository Description\n",
    "* Project_3.ipynb - documentation of steps, calls, and code\n",
    "* docker-compose.yml - configuration of the docker containers used for this analytics pipline\n",
    "* game_api.py - application programming interface to create events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Pipeline Breakdown\n",
    "\n",
    "### Set Up\n",
    "Below we will describe in detail how the pipeline is spun up. First, we need to change into the w205 directory and clone the project repository.\n",
    "\n",
    "```console\n",
    "cd ~/w205\n",
    "git clone https://github.com/mids-w205-martin-mims/project-3-superpeter55.git\n",
    "```\n",
    "\n",
    "Next, we must move into our new directory and create a new branch \"assignemnt\". Using the commands below, we can switch over to that branch and confirm that we are on the assignment branch. \n",
    "\n",
    "```console\n",
    "cd project-3-superpeter55\n",
    "git branch assignment\n",
    "git checkout assignment\n",
    "git status\n",
    "```\n",
    "```console\n",
    "On branch assignment\n",
    "```\n",
    "\n",
    "From there, we'll copy our docker-compose file from week 13 content to run our docker images. The period \".\" at the end of the command copies the selected file into the current directory. From there, we'll update the copied file to ensure our docker containers run properly. We'll do this by: \n",
    "- Commenting out two lines in the cloudera service that read \"ports:\" and \"8888:8888\"\n",
    "- Uncommenting these lines in the spark service, enabling us to run a jupyter notebook with pyspark in port 8888\n",
    "\n",
    "```console\n",
    "cp ~/w205/course-content/13-Understanding-Data/docker-compose.yml .\n",
    "```\n",
    "\n",
    "Now, we will spin up our cluster. The -d at the end runs the containers in the background and we are able to continue to use our console. Next, we will confirm that all the containers are up. If all the containers are in an \"up\" state, we can proceed.\n",
    "\n",
    "```console\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Since our cluster is running, we will create our kafka topic. We will use the exec command on kafka to execute a single kafka command. In this case, we run the kafka-topics command with the --create option to create a topic. The --topic option is used to name the topic \"events\". We chose to name this topic events because we are collecting game event data for this project. We use the --partitions option to specify that we only need one partition. The --if-not-exists option prevents two topics with the same name from being created. The --zookeeper option is specifying which port we would like to connect our topic to. If the console outputs \"Created topic events\", we know that this command has successfully executed.\n",
    "\n",
    "```console\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "Next, we will set up the game_api.py file to define different event types. We will copy the game_api.py file from week 12 folder and then make some modifications, using the command below:\n",
    "\n",
    "```console\n",
    "cp ~/w205/course-content/12-Querying-Data-II/game_api.py .\n",
    "```\n",
    "\n",
    "The game_api.py file from week 12 does not contain the functions, \"buy a sword\" or \"join a guild\", so we will need to create them. For the first function, we give the user the option to buy a sword or to buy a sharp sword. We also have 3 guilds that the user can join; each of which represent a team member on this project. These include: \n",
    "- the Morgan guild\n",
    "- the Lam guild\n",
    "- the Kavlakoglu guild\n",
    "\n",
    "Separate events for each of these guilds have been created for a user to join that specific guild; the generic join_guild event will randomly select a guild to join.\n",
    "\n",
    "These functions are very similar to the function that already exists in the game_api.py file named purchase_a_sword(). The first line of each function specifies the path to trigger each event. More specifically, we use /buy_a_sword or /purchase_a_sharp_sword to trigger the buy sword event. Similarly, to join a guild, we use /join_guild to join a random guild or we use specified guild paths for the user to choose a particular guild. Each of these functions define a dictionary of metadata, and all events have an event type. For buy_sword events, we specify the sword_type as either normal or sharp. For join_guild events, we specify the guild_name, and the assignment_type. The assignment type indicates if users joined the guild randomly or if they chose their guild. The log_to_kafka step sends the event and event metadata to the kafka topic \"events\". Finally, the return statement notifies the user.  \n",
    "\n",
    "```python\n",
    "@app.route(\"/buy_a_sword\")\n",
    "def buy_a_sword():\n",
    "    buy_sword_event = {'event_type': 'buy_sword',\n",
    "                       'sword_type': 'normal'}\n",
    "    log_to_kafka('events', buy_sword_event)\n",
    "    return \"Sword Bought!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild\")\n",
    "def join_guild():\n",
    "    guilds = ['Morgan','Lam','Kavlakoglu']\n",
    "    guild = random.choice(guilds)\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': guild,\n",
    "                        'assignment_type': 'random'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined \" + guild + \" Guild!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_a_sharp_sword\")\n",
    "def purchase_a_sharp_sword():\n",
    "    purchase_sword_event = {'event_type': 'buy_sword',\n",
    "                            'sword_type': 'sharp'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sharp Sword Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_morgan\")\n",
    "def join_guild_morgan():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Morgan',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Morgan Guild!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_lam\")\n",
    "def join_guild_lam():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Lam',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Lam Guild!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_kavlakoglu\")\n",
    "def join_guild_kavlakoglu():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Kavlakoglu',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Kavlakoglu Guild!\\n\"\n",
    "```\n",
    "\n",
    "Now that we have updated our game_api.py file, we can run our flask app through the game_api.py file. The code to run our flask app already exists in the game_api file so we just need to run the file and specify the host which in this case is 0.0.0.0. \n",
    "\n",
    "```console\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-superpeter55/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "Note that the flask app is now running in the window. We will need to open a new terminal to perform commands. When we do this, we need to navigate to the appropriate directory using the following commands.\n",
    "\n",
    "```console\n",
    "cd w205\n",
    "cd project-3-superpeter55/\n",
    "```\n",
    "\n",
    "Next, we want to spin up a jupyter notebook with pyspark, but before we can, we must create a symbolic link to access our mounted w205 directory. The first line uses docker-compose exec to open up a spark bash shell. The second line creates the link using the ln command and specifies the -s option to create a symbolic link. The third line exits and returns us to the shell.\n",
    "\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "ln -s /w205 w205\n",
    "exit \n",
    "```\n",
    "\n",
    "Now we are ready to spin up our notebook using pyspark with the command below. Once again we are using docker-compose exec to run a single spark command. The spark command sets up a notebook in port 8888 and ip address 0.0.0.0. After this command, a URL is returned. In that URL, we replace 0.0.0.0 with our local machine ip address, and we paste the link in our browser to access our notebook.\n",
    "\n",
    "```console\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "\n",
    "Now that we are in our notebook, we want to check that the spark session and spark context are accessible in the notebook. The two code cells below show that spark is properly running in this instance and we are ready to proceed. We will move forward to testing our pipeline in batch mode and generating data with apache bench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc4b3ed6c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our pipeline, we will use Apache Bench to generate events and run them through the pipeline. To test our pipeline, we will generate 3 events of each type from 2 separate users, which we will call, Player_1 and Player_2. Once again, we will use docker compose exec to execute Apache Bench commands below. Each event is very similar in that it calls \"ab\", which stands for Apache Bench. The -n argument specifies how many times we would like to call this event. As you can see below, we call each event 3 times. The -H argument specifies the host or user which we specify as either Player_1 or Player_2. Finally, the path at the end indicates which event from our game_api.py file will be called. \n",
    "\n",
    "We have decided to call every event in the api 3 times across each user to account for all possible actions an individual user can take. Since we have 6 events in our api and 2 users generating test data, it makes sense that we have 12 calls to apache bench specified below. \n",
    "\n",
    "Please Note: you must run each line separately\n",
    "\n",
    "```console\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/purchase_a_sharp_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/purchase_a_sharp_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/buy_a_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/buy_a_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_morgan\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_morgan\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_lam\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_lam\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_kavlakoglu\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_kavlakoglu\n",
    "```\n",
    "\n",
    "Below, we define functions that will determine if each event is a purchase event, join event, or default event and these will be used to classify events both in batch mode and later in streaming mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'buy_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_default(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'default':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will read in our raw events that we have generated from Apache bench and then show them to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     0|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     1|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     2|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     3|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     4|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     5|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     6|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     7|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     8|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     9|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    10|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    11|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    12|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    13|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    14|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    15|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    16|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    17|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    18|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    19|2021-12-08 03:33:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need most of the information above, we really just need the value and timestamp. We use select to pull these values out and we use the is_purchase function defined above to only get the purchased events for analysis. We then show the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchase_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_purchase('raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchase_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to unpack the json in the raw column which is done below. The following cell shows the resulting schema and the cell after that shows the what the dataframe looks like. We have 12 events as expected because we have 2 players, each buying 3 normal swords and 3 sharp swords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_purchase_events = purchase_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- sword_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_purchase_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_purchase_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write this data to parquet so it can be be queryable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_purchase_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to repeat this process with both join events and default events. The process for this is identical except we use is_join to filter for join events and is_default to filter for default events. The first cell filters the raw events to only include guild joins and shows these events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_join('raw'))\n",
    "        \n",
    "join_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we unwrap the join events json, print the schema and show the data. You can see that when the assignment type is random the guild the user joins varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- assignment_type: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- guild_name: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_join_events = join_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "        \n",
    "extracted_join_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_join_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are writing our join data to parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_join_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/joins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are on the default events. Like before, we filter out the raw data to only include our default events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_default('raw'))\n",
    "        \n",
    "default_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we unwrap the json format of our default events, print the schema and print our data. As expected we have 6 entries since we generated 3 events for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_default_events = default_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "        \n",
    "extracted_default_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|           timestamp|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_default_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write our data to parquet format in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_default_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/defaults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written our data to parquet. We will read it in and do some basic queries to ensure our pipeline is functioning properly. First we read in our purchase data from HDFS and register a temporary table to perform spark queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases = spark.read.parquet('/tmp/purchases')\n",
    "purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchases.registerTempTable('purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform a simple query to show it is working. In this case we selecting all purchase data from player 1 and as expected we have 6 entries. Three instances of buying a normal sword and 3 instances of buying a sharp sword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_player1 = spark.sql(\"select * from purchases where host='Player_1'\")\n",
    "purchases_player1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat the process for joins. Below we read in our guild join data from HDFS and show it. This is followed by registering the data as a temp table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joins = spark.read.parquet('/tmp/joins')\n",
    "joins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joins.registerTempTable('joins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will query our data to only show entries for player 2. Interestingly, on player 2's random guild joins. They joined each guild once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joins_player2 = spark.sql(\"select * from joins where host='Player_2'\")\n",
    "joins_player2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we finish up the process with reading in the default events and showing them. It is time to move on to streaming mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|           timestamp|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "defaults = spark.read.parquet('/tmp/defaults')\n",
    "defaults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Streaming Mode\n",
    "### Setup Event Catching\n",
    "\n",
    "Before we begin streaming. We must define the schema of our events. We have 3 separate schema. One for purchase events, one for guild join events, and default event schema. This is done below. Note that we don't include timestamps in these schema because this schema is only defining what we need to catch from our json data and the timestamp is separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- sword_type: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"sword_type\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def join_guild_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- assignment_type: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- guild_name: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"assignment_type\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"guild_name\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def default_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses spark to read our events in from our kafka server. Note that we have not started streaming events now but we are now ready to catch them. The cell below describes which raw data we will catch from each type of event (purchase, join, default). In each case we feed the json schema defined above and the timestamp to complete the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sword_purchases = raw_events \\\n",
    "        .filter(is_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          purchase_sword_event_schema()).alias('json')) \\\n",
    "        .select('json.*','timestamp')\n",
    "        \n",
    "guild_joins = raw_events \\\n",
    "        .filter(is_join(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          join_guild_event_schema()).alias('json')) \\\n",
    "        .select('json.*','timestamp')\n",
    "        \n",
    "defaults = raw_events \\\n",
    "        .filter(is_default(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          default_event_schema()).alias('json')) \\\n",
    "        .select('json.*','timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print all the schema to ensure it looks the same as in batch mode which it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- sword_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sword_purchases.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- assignment_type: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- guild_name: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "guild_joins.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "defaults.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will start a sink for each type of event. This actively writes our data to HDFS every 10 seconds. Once again, we have not fed any data yet so right now it will just be blank parquet files but once we begin streaming it will write to HDFS all the data we are streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sink_sword = sword_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "        \n",
    "sink_guild = guild_joins \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_guild_joins\") \\\n",
    "        .option(\"path\", \"/tmp/guild_joins\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "        \n",
    "sink_default = defaults \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_defaults\") \\\n",
    "        .option(\"path\", \"/tmp/defaults\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Events\n",
    "\n",
    "Open another terminal window, run while loop to continuously generate events. In this case, each player is going to perform every event the api has to offer once every 10 seconds. Once again, Apache Bench is used to generate the events. The only difference between the streaming version and the batch version is that we are specifying each event to only happen once. Additionally, we are also going to run each bench command once every 10 seconds; in this way, we are continuously streaming data.\n",
    "\n",
    "```console\n",
    "while true; do\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/purchase_a_sharp_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/purchase_a_sharp_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/buy_a_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/buy_a_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_morgan\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_morgan\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_lam\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_lam\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_kavlakoglu\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_kavlakoglu\n",
    "  sleep 10\n",
    "done\n",
    "```\n",
    "\n",
    "Now, we are ready to start Hive to define our schema. This is done below.\n",
    "\n",
    "```console\n",
    "docker-compose exec cloudera hive\n",
    "```\n",
    "\n",
    "In hive, we run the commands below to set up the schema for our tables in presto.\n",
    "\n",
    "```console\n",
    "create external table if not exists default.sword_purchases (Accept string, Host string, `User-Agent` string, event_type string, sword_type string, timestamp string) stored as parquet location '/tmp/sword_purchases'  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "\n",
    "create external table if not exists default.guild_joins (Accept string, Host string, `User-Agent` string, assignment_type string, event_type string, guild_name string, timestamp string) stored as parquet location '/tmp/guild_joins'  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "\n",
    "create external table if not exists default.defaults (Accept string, Host string, `User-Agent` string, event_type string, timestamp string) stored as parquet location '/tmp/defaults'  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "Now, we exit Hive and startup Presto\n",
    "\n",
    "```console\n",
    "exit;\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "\n",
    "We will run a few test queires to ensure the data has landed in presto correctly. First, we look at the first 5 rows of each table we have and make sure they look correct which they do.\n",
    "\n",
    "```console\n",
    "select * from sword_purchases limit 5;\n",
    "```\n",
    "```console\n",
    " accept |   host   |   user-agent    | event_type | sword_type |        timestamp        \n",
    "--------+----------+-----------------+------------+------------+-------------------------\n",
    " */*    | Player_1 | ApacheBench/2.3 | buy_sword  | sharp      | 2021-12-09 14:24:01.578 \n",
    " */*    | Player_2 | ApacheBench/2.3 | buy_sword  | sharp      | 2021-12-09 14:24:02.376 \n",
    " */*    | Player_1 | ApacheBench/2.3 | buy_sword  | normal     | 2021-12-09 14:24:02.856 \n",
    " */*    | Player_2 | ApacheBench/2.3 | buy_sword  | normal     | 2021-12-09 14:24:03.35  \n",
    " */*    | Player_1 | ApacheBench/2.3 | buy_sword  | sharp      | 2021-12-09 14:25:47.571 \n",
    "(5 rows)\n",
    "```\n",
    "```console\n",
    "select * from guild_joins limit 5;\n",
    "```\n",
    "```console\n",
    " accept |   host   |   user-agent    | assignment_type | event_type | guild_name |        timestamp        \n",
    "--------+----------+-----------------+-----------------+------------+------------+-------------------------\n",
    " */*    | Player_1 | ApacheBench/2.3 | manual          | join_guild | Kavlakoglu | 2021-12-09 14:29:30.718 \n",
    " */*    | Player_1 | ApacheBench/2.3 | random          | join_guild | Kavlakoglu | 2021-12-09 14:24:21.025 \n",
    " */*    | Player_2 | ApacheBench/2.3 | random          | join_guild | Kavlakoglu | 2021-12-09 14:24:22.19  \n",
    " */*    | Player_1 | ApacheBench/2.3 | manual          | join_guild | Morgan     | 2021-12-09 14:24:22.931 \n",
    " */*    | Player_2 | ApacheBench/2.3 | manual          | join_guild | Morgan     | 2021-12-09 14:24:23.399 \n",
    "(5 rows)\n",
    "```\n",
    "```console\n",
    "select * from defaults limit 5;\n",
    "```\n",
    "```console\n",
    " accept |   host   |   user-agent    | event_type |        timestamp        \n",
    "--------+----------+-----------------+------------+-------------------------\n",
    " */*    | Player_1 | ApacheBench/2.3 | default    | 2021-12-09 14:23:59.827 \n",
    " */*    | Player_1 | ApacheBench/2.3 | default    | 2021-12-09 14:29:41.517 \n",
    " */*    | Player_2 | ApacheBench/2.3 | default    | 2021-12-09 14:29:41.96  \n",
    " */*    | Player_2 | ApacheBench/2.3 | default    | 2021-12-09 14:24:00.601 \n",
    " */*    | Player_1 | ApacheBench/2.3 | default    | 2021-12-09 14:30:35.119 \n",
    "(5 rows)\n",
    "```\n",
    "\n",
    "Next, we will test to see if streaming works. To do this, we run queries to count the number of rows. We then will wait a few minutes and run the same queries again to ensure the number of events has grown in each table. The initial set of queries is below.\n",
    "\n",
    "```console\n",
    "select count(accept) as num_rows_purchases \n",
    "       from sword_purchases;\n",
    "```\n",
    "```console\n",
    " num_rows_purchases \n",
    "--------------------\n",
    "                216 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_joins \n",
    "       from guild_joins;\n",
    "```\n",
    "```console\n",
    " num_rows_joins \n",
    "----------------\n",
    "            448 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_defaults \n",
    "       from defaults;\n",
    "```\n",
    "```console\n",
    " num_rows_defaults \n",
    "-------------------\n",
    "               116 \n",
    "(1 row)\n",
    "```\n",
    "After a few minutes, we run the queries again and can see that the amount of data has increased in each table as expected. It appears that streaming is working so it is time to move on to our analytics questions.\n",
    "```console\n",
    "select count(accept) as num_rows \n",
    "       from sword_purchases;\n",
    "```\n",
    "```console\n",
    " num_rows \n",
    "----------\n",
    "      248 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_joins \n",
    "       from guild_joins;\n",
    "```\n",
    "```console\n",
    " num_rows_joins \n",
    "----------------\n",
    "            504 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_defaults \n",
    "       from defaults;\n",
    "```\n",
    "```console\n",
    " num_rows_defaults \n",
    "-------------------\n",
    "               128 \n",
    "(1 row)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics Questions\n",
    "\n",
    "First we stop streaming and then we will perform our analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sink_sword.stop()\n",
    "sink_guild.stop()\n",
    "sink_default.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first set of queries counts the final size of our tables.\n",
    "\n",
    "```console\n",
    "select count(accept) as num_rows \n",
    "       from sword_purchases;\n",
    "```\n",
    "```console\n",
    " num_rows \n",
    "----------\n",
    "      308 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_joins \n",
    "       from guild_joins;\n",
    "```\n",
    "```console\n",
    " num_rows_joins \n",
    "----------------\n",
    "            616 \n",
    "(1 row)\n",
    "```\n",
    "```console\n",
    "select count(accept) as num_rows_defaults \n",
    "       from defaults;\n",
    "```\n",
    "```console\n",
    " num_rows_defaults \n",
    "-------------------\n",
    "               154 \n",
    "(1 row)\n",
    "```\n",
    "\n",
    "### Sword Sale Summary:\n",
    "\n",
    "For our sword sale summary, we would like to know which type of sword is more popolar and the total number of swords purchased. We would expect no preference to a certain type of sword since the events are streamed at the same rate.\n",
    "\n",
    "Most popular sword:\n",
    "\n",
    " ```console\n",
    " select sword_type, count(sword_type) as amount \n",
    "        from sword_purchases \n",
    "        group by sword_type;\n",
    " ```\n",
    " ```console\n",
    " sword_type | amount \n",
    "------------+--------\n",
    " sharp      |    154 \n",
    " normal     |    154 \n",
    "(2 rows)\n",
    " ```\n",
    "\n",
    "Total swords purchased:\n",
    "\n",
    " ```console\n",
    " select count(accept) as num_swords_purchased \n",
    "        from sword_purchases;\n",
    " ```\n",
    " ```console\n",
    " num_swords_purchased \n",
    "----------------------\n",
    "                  308 \n",
    "(1 row)\n",
    " ```\n",
    "\n",
    "### Guild Summary:\n",
    "\n",
    " For our guild summary, our first question we would like to answer is how many members each guild has. As you can see the Lam guild has the most members followed by the Morgan guild and the Kavlakoglu guild has the fewest members.\n",
    " \n",
    " ```console\n",
    " select guild_name, count(guild_name) as members \n",
    "        from guild_joins \n",
    "        group by guild_name \n",
    "        order by members desc;\n",
    " ```\n",
    " ```console\n",
    " guild_name | members \n",
    "------------+---------\n",
    " Lam        |     216 \n",
    " Morgan     |     201 \n",
    " Kavlakoglu |     199 \n",
    "(3 rows)\n",
    " ```\n",
    " \n",
    " Next, we would like to know how many members each guild has by specifying the guild they would like to join. We would expect this to be even since we stream in the same amount of joins for each memeber. We can see below that the number of manually joined member is even as expected.\n",
    " \n",
    " ```console\n",
    " select guild_name, count(guild_name) as manually_joined_members \n",
    "        from guild_joins \n",
    "        where assignment_type = 'manual' \n",
    "        group by guild_name \n",
    "        order by manually_joined_members desc;\n",
    " ```\n",
    " ```console\n",
    " guild_name | manually_joined_members \n",
    "------------+-------------------------\n",
    " Morgan     |                     154 \n",
    " Lam        |                     154 \n",
    " Kavlakoglu |                     154 \n",
    "(3 rows)\n",
    " ```\n",
    " \n",
    " We would now like to know how many members each guild has by random assignment. You can see that the Lam guild had the most members randomly assigned followed by the Morgan guild and finally the Kavlakoglu guild. This matches up with the total guild members as expected.\n",
    " \n",
    " ```console\n",
    " select guild_name, count(guild_name) as randomly_joined_members \n",
    "        from guild_joins \n",
    "        where assignment_type = 'random' \n",
    "        group by guild_name \n",
    "        order by randomly_joined_members desc;\n",
    " ```\n",
    " ```console\n",
    " guild_name | randomly_joined_members \n",
    "------------+-------------------------\n",
    " Lam        |                      62 \n",
    " Morgan     |                      47 \n",
    " Kavlakoglu |                      45 \n",
    "(3 rows)\n",
    " ```\n",
    " \n",
    " Finally, we see how many guild members were assigned to each guild by player 1 and player 2. Both players favored the Lam guild while player 2 favored the Lam guild more heavily.\n",
    " \n",
    " ```console\n",
    " select host as player, count(case when guild_name = 'Morgan' then 1 end) as morgan_joins,\n",
    "        count(case when guild_name = 'Lam' then 1 end) as lam_joins,\n",
    "        count(case when guild_name = 'Kavlakoglu' then 1 end) as kavlakoglu_joins\n",
    "        from guild_joins group by host;\n",
    " ```\n",
    " ```console\n",
    "  player  | morgan_joins | lam_joins | kavlakoglu_joins \n",
    "----------+--------------+-----------+------------------\n",
    " Player_1 |          101 |       106 |              101 \n",
    " Player_2 |          100 |       110 |               98 \n",
    "(2 rows)\n",
    " ```\n",
    " \n",
    " Gramercy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
